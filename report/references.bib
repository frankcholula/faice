@online{chiDiffusionPolicyVisuomotor2024,
  title = {Diffusion {{Policy}}: {{Visuomotor Policy Learning}} via {{Action Diffusion}}},
  shorttitle = {Diffusion {{Policy}}},
  author = {Chi, Cheng and Xu, Zhenjia and Feng, Siyuan and Cousineau, Eric and Du, Yilun and Burchfiel, Benjamin and Tedrake, Russ and Song, Shuran},
  date = {2024-03-14},
  eprint = {2303.04137},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.04137},
  url = {http://arxiv.org/abs/2303.04137},
  urldate = {2025-05-03},
  abstract = {This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9\%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details is publicly available diffusion-policy.cs.columbia.edu},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics,notion},
  file = {/Users/frankcholula/Zotero/storage/GC8ATPTT/Chi et al. - 2024 - Diffusion Policy Visuomotor Policy Learning via Action Diffusion.pdf;/Users/frankcholula/Zotero/storage/6FZ8LAUN/2303.html}
}

@online{dhariwalDiffusionModelsBeat2021,
  title = {Diffusion {{Models Beat GANs}} on {{Image Synthesis}}},
  author = {Dhariwal, Prafulla and Nichol, Alex},
  date = {2021-06-01},
  eprint = {2105.05233},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2105.05233},
  url = {http://arxiv.org/abs/2105.05233},
  urldate = {2025-04-03},
  abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\$\textbackslash times\$128, 4.59 on ImageNet 256\$\textbackslash times\$256, and 7.72 on ImageNet 512\$\textbackslash times\$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256\$\textbackslash times\$256 and 3.85 on ImageNet 512\$\textbackslash times\$512. We release our code at https://github.com/openai/guided-diffusion},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/frankcholula/Zotero/storage/2WVEV7XA/Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf;/Users/frankcholula/Zotero/storage/HGVDPJAW/2105.html}
}

@online{dosovitskiyImageWorth16x162021a,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  eprint = {2010.11929},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.11929},
  url = {http://arxiv.org/abs/2010.11929},
  urldate = {2025-05-07},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion},
  file = {/Users/frankcholula/Zotero/storage/5TB9QJWP/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf;/Users/frankcholula/Zotero/storage/6B36AS6T/2010.html}
}

@online{heuselGANsTrainedTwo2018,
  title = {{{GANs Trained}} by a {{Two Time-Scale Update Rule Converge}} to a {{Local Nash Equilibrium}}},
  author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  date = {2018-01-12},
  eprint = {1706.08500},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.08500},
  url = {http://arxiv.org/abs/1706.08500},
  urldate = {2025-05-07},
  abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr\textbackslash 'echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/frankcholula/Zotero/storage/ZA432BZK/Heusel et al. - 2018 - GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.pdf;/Users/frankcholula/Zotero/storage/66PJJ2BJ/1706.html}
}

@online{hoClassifierFreeDiffusionGuidance2022,
  title = {Classifier-{{Free Diffusion Guidance}}},
  author = {Ho, Jonathan and Salimans, Tim},
  date = {2022-07-26},
  eprint = {2207.12598},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.12598},
  url = {http://arxiv.org/abs/2207.12598},
  urldate = {2025-04-03},
  abstract = {Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,notion},
  file = {/Users/frankcholula/Zotero/storage/EGVA9L9V/Ho and Salimans - 2022 - Classifier-Free Diffusion Guidance.pdf;/Users/frankcholula/Zotero/storage/5CR64HC8/2207.html}
}

@online{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  date = {2020-12-16},
  eprint = {2006.11239},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2006.11239},
  url = {http://arxiv.org/abs/2006.11239},
  urldate = {2025-04-03},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/frankcholula/Zotero/storage/6FD8IGAD/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf;/Users/frankcholula/Zotero/storage/SU3D7HWD/2006.html}
}

@online{huiNeuralWaveletdomainDiffusion2022,
  title = {Neural {{Wavelet-domain Diffusion}} for {{3D Shape Generation}}},
  author = {Hui, Ka-Hei and Li, Ruihui and Hu, Jingyu and Fu, Chi-Wing},
  date = {2022-09-19},
  eprint = {2209.08725},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2209.08725},
  url = {http://arxiv.org/abs/2209.08725},
  urldate = {2025-05-04},
  abstract = {This paper presents a new approach for 3D shape generation, enabling direct generative modeling on a continuous implicit representation in wavelet domain. Specifically, we propose a compact wavelet representation with a pair of coarse and detail coefficient volumes to implicitly represent 3D shapes via truncated signed distance functions and multi-scale biorthogonal wavelets, and formulate a pair of neural networks: a generator based on the diffusion model to produce diverse shapes in the form of coarse coefficient volumes; and a detail predictor to further produce compatible detail coefficient volumes for enriching the generated shapes with fine structures and details. Both quantitative and qualitative experimental results manifest the superiority of our approach in generating diverse and high-quality shapes with complex topology and structures, clean surfaces, and fine details, exceeding the 3D generation capabilities of the state-of-the-art models.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,notion},
  file = {/Users/frankcholula/Zotero/storage/2UBU5NDP/Hui et al. - 2022 - Neural Wavelet-domain Diffusion for 3D Shape Generation.pdf;/Users/frankcholula/Zotero/storage/ULPSLYPB/2209.html}
}

@online{karrasElucidatingDesignSpace2022,
  title = {Elucidating the {{Design Space}} of {{Diffusion-Based Generative Models}}},
  author = {Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
  date = {2022-10-11},
  eprint = {2206.00364},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.00364},
  url = {http://arxiv.org/abs/2206.00364},
  urldate = {2025-04-27},
  abstract = {We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,notion,Statistics - Machine Learning},
  file = {/Users/frankcholula/Zotero/storage/N6NUYUQM/Karras et al. - 2022 - Elucidating the Design Space of Diffusion-Based Generative Models.pdf;/Users/frankcholula/Zotero/storage/HMRSJ2G2/2206.html}
}

@online{kingmaAutoEncodingVariationalBayes2022,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2022-12-10},
  eprint = {1312.6114},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1312.6114},
  url = {http://arxiv.org/abs/1312.6114},
  urldate = {2025-04-26},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/frankcholula/Zotero/storage/6G9AHDFK/Kingma and Welling - 2022 - Auto-Encoding Variational Bayes.pdf;/Users/frankcholula/Zotero/storage/KCPPT9UX/1312.html}
}

@online{linCommonDiffusionNoise2024,
  title = {Common {{Diffusion Noise Schedules}} and {{Sample Steps}} Are {{Flawed}}},
  author = {Lin, Shanchuan and Liu, Bingchen and Li, Jiashi and Yang, Xiao},
  date = {2024-01-23},
  eprint = {2305.08891},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.08891},
  url = {http://arxiv.org/abs/2305.08891},
  urldate = {2025-04-28},
  abstract = {We discover that common diffusion noise schedules do not enforce the last timestep to have zero signal-to-noise ratio (SNR), and some implementations of diffusion samplers do not start from the last timestep. Such designs are flawed and do not reflect the fact that the model is given pure Gaussian noise at inference, creating a discrepancy between training and inference. We show that the flawed design causes real problems in existing implementations. In Stable Diffusion, it severely limits the model to only generate images with medium brightness and prevents it from generating very bright and dark samples. We propose a few simple fixes: (1) rescale the noise schedule to enforce zero terminal SNR; (2) train the model with v prediction; (3) change the sampler to always start from the last timestep; (4) rescale classifier-free guidance to prevent over-exposure. These simple changes ensure the diffusion process is congruent between training and inference and allow the model to generate samples more faithful to the original data distribution.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {/Users/frankcholula/Zotero/storage/BZTLSTKM/Lin et al. - 2024 - Common Diffusion Noise Schedules and Sample Steps are Flawed.pdf;/Users/frankcholula/Zotero/storage/X8SGVMYJ/2305.html}
}

@online{lipmanFlowMatchingGenerative2023,
  title = {Flow {{Matching}} for {{Generative Modeling}}},
  author = {Lipman, Yaron and Chen, Ricky T. Q. and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
  date = {2023-02-08},
  eprint = {2210.02747},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.02747},
  url = {http://arxiv.org/abs/2210.02747},
  urldate = {2025-04-07},
  abstract = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/frankcholula/Zotero/storage/HXVL9ULC/Lipman et al. - 2023 - Flow Matching for Generative Modeling.pdf;/Users/frankcholula/Zotero/storage/Z5F47GUS/2210.html}
}

@online{lipmanFlowMatchingGuide2024,
  title = {Flow {{Matching Guide}} and {{Code}}},
  author = {Lipman, Yaron and Havasi, Marton and Holderrieth, Peter and Shaul, Neta and Le, Matt and Karrer, Brian and Chen, Ricky T. Q. and Lopez-Paz, David and Ben-Hamu, Heli and Gat, Itai},
  date = {2024-12-09},
  eprint = {2412.06264},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2412.06264},
  url = {http://arxiv.org/abs/2412.06264},
  urldate = {2025-05-04},
  abstract = {Flow Matching (FM) is a recent framework for generative modeling that has achieved state-of-the-art performance across various domains, including image, video, audio, speech, and biological structures. This guide offers a comprehensive and self-contained review of FM, covering its mathematical foundations, design choices, and extensions. By also providing a PyTorch package featuring relevant examples (e.g., image and text generation), this work aims to serve as a resource for both novice and experienced researchers interested in understanding, applying and further developing FM.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,notion},
  file = {/Users/frankcholula/Zotero/storage/6FY5H7ZP/Lipman et al. - 2024 - Flow Matching Guide and Code.pdf;/Users/frankcholula/Zotero/storage/3C9WLKXN/2412.html}
}

@online{liuPseudoNumericalMethods2022,
  title = {Pseudo {{Numerical Methods}} for {{Diffusion Models}} on {{Manifolds}}},
  author = {Liu, Luping and Ren, Yi and Lin, Zhijie and Zhao, Zhou},
  date = {2022-10-31},
  eprint = {2202.09778},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2202.09778},
  url = {http://arxiv.org/abs/2202.09778},
  urldate = {2025-04-22},
  abstract = {Denoising Diffusion Probabilistic Models (DDPMs) can generate high-quality samples such as image and audio samples. However, DDPMs require hundreds to thousands of iterations to produce final samples. Several prior works have successfully accelerated DDPMs through adjusting the variance schedule (e.g., Improved Denoising Diffusion Probabilistic Models) or the denoising equation (e.g., Denoising Diffusion Implicit Models (DDIMs)). However, these acceleration methods cannot maintain the quality of samples and even introduce new noise at a high speedup rate, which limit their practicability. To accelerate the inference process while keeping the sample quality, we provide a fresh perspective that DDPMs should be treated as solving differential equations on manifolds. Under such a perspective, we propose pseudo numerical methods for diffusion models (PNDMs). Specifically, we figure out how to solve differential equations on manifolds and show that DDIMs are simple cases of pseudo numerical methods. We change several classical numerical methods to corresponding pseudo numerical methods and find that the pseudo linear multi-step method is the best in most situations. According to our experiments, by directly using pre-trained models on Cifar10, CelebA and LSUN, PNDMs can generate higher quality synthetic images with only 50 steps compared with 1000-step DDIMs (20x speedup), significantly outperform DDIMs with 250 steps (by around 0.4 in FID) and have good generalization on different variance schedules. Our implementation is available at https://github.com/luping-liu/PNDM.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Numerical Analysis,Mathematics - Numerical Analysis,notion,Statistics - Machine Learning},
  file = {/Users/frankcholula/Zotero/storage/T4BBDC56/Liu et al. - 2022 - Pseudo Numerical Methods for Diffusion Models on Manifolds.pdf;/Users/frankcholula/Zotero/storage/63P3E3RW/2202.html}
}

@online{nicholGLIDEPhotorealisticImage2022,
  title = {{{GLIDE}}: {{Towards Photorealistic Image Generation}} and {{Editing}} with {{Text-Guided Diffusion Models}}},
  shorttitle = {{{GLIDE}}},
  author = {Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark},
  date = {2022-03-08},
  eprint = {2112.10741},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2112.10741},
  url = {http://arxiv.org/abs/2112.10741},
  urldate = {2025-04-03},
  abstract = {Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,notion},
  file = {/Users/frankcholula/Zotero/storage/T4DBBU7S/Nichol et al. - 2022 - GLIDE Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models.pdf;/Users/frankcholula/Zotero/storage/629EFL6I/2112.html}
}

@online{nicholImprovedDenoisingDiffusion2021,
  title = {Improved {{Denoising Diffusion Probabilistic Models}}},
  author = {Nichol, Alex and Dhariwal, Prafulla},
  date = {2021-02-18},
  eprint = {2102.09672},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2102.09672},
  url = {http://arxiv.org/abs/2102.09672},
  urldate = {2025-04-22},
  abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/frankcholula/Zotero/storage/5UP32A3W/Nichol and Dhariwal - 2021 - Improved Denoising Diffusion Probabilistic Models.pdf;/Users/frankcholula/Zotero/storage/W2MBBTPT/2102.html}
}

@online{peeblesScalableDiffusionModels2023,
  title = {Scalable {{Diffusion Models}} with {{Transformers}}},
  author = {Peebles, William and Xie, Saining},
  date = {2023-03-02},
  eprint = {2212.09748},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.09748},
  url = {http://arxiv.org/abs/2212.09748},
  urldate = {2025-04-26},
  abstract = {We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion},
  file = {/Users/frankcholula/Zotero/storage/MYWWLH4I/Peebles and Xie - 2023 - Scalable Diffusion Models with Transformers.pdf;/Users/frankcholula/Zotero/storage/WT7HR78N/2212.html}
}

@online{psenkaLearningDiffusionModel2025,
  title = {Learning a {{Diffusion Model Policy}} from {{Rewards}} via {{Q-Score Matching}}},
  author = {Psenka, Michael and Escontrela, Alejandro and Abbeel, Pieter and Ma, Yi},
  date = {2025-02-13},
  eprint = {2312.11752},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.11752},
  url = {http://arxiv.org/abs/2312.11752},
  urldate = {2025-05-03},
  abstract = {Diffusion models have become a popular choice for representing actor policies in behavior cloning and offline reinforcement learning. This is due to their natural ability to optimize an expressive class of distributions over a continuous space. However, previous works fail to exploit the score-based structure of diffusion models, and instead utilize a simple behavior cloning term to train the actor, limiting their ability in the actor-critic setting. In this paper, we present a theoretical framework linking the structure of diffusion model policies to a learned Q-function, by linking the structure between the score of the policy to the action gradient of the Q-function. We focus on off-policy reinforcement learning and propose a new policy update method from this theory, which we denote Q-score matching. Notably, this algorithm only needs to differentiate through the denoising model rather than the entire diffusion model evaluation, and converged policies through Q-score matching are implicitly multi-modal and explorative in continuous domains. We conduct experiments in simulated environments to demonstrate the viability of our proposed method and compare to popular baselines. Source code is available from the project website: https://michaelpsenka.io/qsm.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,notion},
  file = {/Users/frankcholula/Zotero/storage/VA4YJMDN/Psenka et al. - 2025 - Learning a Diffusion Model Policy from Rewards via Q-Score Matching.pdf;/Users/frankcholula/Zotero/storage/NEJNE3PH/2312.html}
}

@online{rombachHighResolutionImageSynthesis2022,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj√∂rn},
  date = {2022-04-13},
  eprint = {2112.10752},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2112.10752},
  url = {http://arxiv.org/abs/2112.10752},
  urldate = {2025-04-26},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {/Users/frankcholula/Zotero/storage/6NYUCP5C/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffusion Models.pdf;/Users/frankcholula/Zotero/storage/BML59C64/2112.html}
}

@online{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  date = {2015-05-18},
  eprint = {1505.04597},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1505.04597},
  url = {http://arxiv.org/abs/1505.04597},
  urldate = {2025-05-07},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/frankcholula/Zotero/storage/7DZ5LJR7/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf;/Users/frankcholula/Zotero/storage/8E3MCGDR/1505.html}
}

@online{sahariaPhotorealisticTexttoImageDiffusion2022,
  title = {Photorealistic {{Text-to-Image Diffusion Models}} with {{Deep Language Understanding}}},
  author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho, Jonathan and Fleet, David J. and Norouzi, Mohammad},
  date = {2022-05-23},
  eprint = {2205.11487},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.11487},
  url = {http://arxiv.org/abs/2205.11487},
  urldate = {2025-05-08},
  abstract = {We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion},
  file = {/Users/frankcholula/Zotero/storage/7LLVMHDD/Saharia et al. - 2022 - Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.pdf;/Users/frankcholula/Zotero/storage/V88ZA5QC/2205.html}
}

@online{salimansImprovedTechniquesTraining2016,
  title = {Improved {{Techniques}} for {{Training GANs}}},
  author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  date = {2016-06-10},
  eprint = {1606.03498},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1606.03498},
  url = {http://arxiv.org/abs/1606.03498},
  urldate = {2025-05-07},
  abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,notion},
  file = {/Users/frankcholula/Zotero/storage/M5DHR9MU/Salimans et al. - 2016 - Improved Techniques for Training GANs.pdf;/Users/frankcholula/Zotero/storage/HD2AK9WI/1606.html}
}

@online{salimansProgressiveDistillationFast2022,
  title = {Progressive {{Distillation}} for {{Fast Sampling}} of {{Diffusion Models}}},
  author = {Salimans, Tim and Ho, Jonathan},
  date = {2022-06-07},
  eprint = {2202.00512},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2202.00512},
  url = {http://arxiv.org/abs/2202.00512},
  urldate = {2025-05-07},
  abstract = {Diffusion models have recently shown great promise for generative modeling, outperforming GANs on perceptual quality and autoregressive models at density estimation. A remaining downside is their slow sampling time: generating high quality samples takes many hundreds or thousands of model evaluations. Here we make two contributions to help eliminate this downside: First, we present new parameterizations of diffusion models that provide increased stability when using few sampling steps. Second, we present a method to distill a trained deterministic diffusion sampler, using many steps, into a new diffusion model that takes half as many sampling steps. We then keep progressively applying this distillation procedure to our model, halving the number of required sampling steps each time. On standard image generation benchmarks like CIFAR-10, ImageNet, and LSUN, we start out with state-of-the-art samplers taking as many as 8192 steps, and are able to distill down to models taking as few as 4 steps without losing much perceptual quality; achieving, for example, a FID of 3.0 on CIFAR-10 in 4 steps. Finally, we show that the full progressive distillation procedure does not take more time than it takes to train the original model, thus representing an efficient solution for generative modeling using diffusion at both train and test time.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/frankcholula/Zotero/storage/CWMLYF56/Salimans and Ho - 2022 - Progressive Distillation for Fast Sampling of Diffusion Models.pdf;/Users/frankcholula/Zotero/storage/BCIXCXV3/2202.html}
}

@online{sohl-dicksteinDeepUnsupervisedLearning2015,
  title = {Deep {{Unsupervised Learning}} Using {{Nonequilibrium Thermodynamics}}},
  author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  date = {2015-11-18},
  eprint = {1503.03585},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1503.03585},
  url = {http://arxiv.org/abs/1503.03585},
  urldate = {2025-04-03},
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,notion,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/frankcholula/Zotero/storage/YM6Z9QJC/Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Thermodynamics.pdf;/Users/frankcholula/Zotero/storage/6YKR5BGG/1503.html}
}

@online{songConsistencyModels2023,
  title = {Consistency {{Models}}},
  author = {Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya},
  date = {2023-05-31},
  eprint = {2303.01469},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.01469},
  url = {http://arxiv.org/abs/2303.01469},
  urldate = {2025-04-26},
  abstract = {Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64x64 and LSUN 256x256.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/frankcholula/Zotero/storage/IE7ALX88/Song et al. - 2023 - Consistency Models.pdf;/Users/frankcholula/Zotero/storage/CLFTMYZV/2303.html}
}

@online{songDenoisingDiffusionImplicit2022,
  title = {Denoising {{Diffusion Implicit Models}}},
  author = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  date = {2022-10-05},
  eprint = {2010.02502},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.02502},
  url = {http://arxiv.org/abs/2010.02502},
  urldate = {2025-04-22},
  abstract = {Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples \$10 \textbackslash times\$ to \$50 \textbackslash times\$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion},
  file = {/Users/frankcholula/Zotero/storage/IN2XW67J/Song et al. - 2022 - Denoising Diffusion Implicit Models.pdf;/Users/frankcholula/Zotero/storage/NW2I3BA7/2010.html}
}

@online{songGenerativeModelingEstimating2020,
  title = {Generative {{Modeling}} by {{Estimating Gradients}} of the {{Data Distribution}}},
  author = {Song, Yang and Ermon, Stefano},
  date = {2020-10-10},
  eprint = {1907.05600},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1907.05600},
  url = {http://arxiv.org/abs/1907.05600},
  urldate = {2025-04-03},
  abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/frankcholula/Zotero/storage/Q4XUUUF7/Song and Ermon - 2020 - Generative Modeling by Estimating Gradients of the Data Distribution.pdf;/Users/frankcholula/Zotero/storage/DEDMV2SQ/1907.html}
}

@online{songScoreBasedGenerativeModeling2021,
  title = {Score-{{Based Generative Modeling}} through {{Stochastic Differential Equations}}},
  author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  date = {2021-02-10},
  eprint = {2011.13456},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2011.13456},
  url = {http://arxiv.org/abs/2011.13456},
  urldate = {2025-04-26},
  abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 ÀÜ 1024 images for the first time from a score-based generative model.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/frankcholula/Zotero/storage/GW37Y3QS/Song et al. - 2021 - Score-Based Generative Modeling through Stochastic Differential Equations.pdf}
}

@online{valevskiDiffusionModelsAre2025,
  title = {Diffusion {{Models Are Real-Time Game Engines}}},
  author = {Valevski, Dani and Leviathan, Yaniv and Arar, Moab and Fruchter, Shlomi},
  date = {2025-04-24},
  eprint = {2408.14837},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2408.14837},
  url = {http://arxiv.org/abs/2408.14837},
  urldate = {2025-05-07},
  abstract = {We present GameNGen, the first game engine powered entirely by a neural model that also enables real-time interaction with a complex environment over long trajectories at high quality. When trained on the classic game DOOM, GameNGen extracts gameplay and uses it to generate a playable environment that can interactively simulate new trajectories. GameNGen runs at 20 frames per second on a single TPU and remains stable over extended multi-minute play sessions. Next frame prediction achieves a PSNR of 29.4, comparable to lossy JPEG compression. Human raters are only slightly better than random chance at distinguishing short clips of the game from clips of the simulation, even after 5 minutes of auto-regressive generation. GameNGen is trained in two phases: (1) an RL-agent learns to play the game and the training sessions are recorded, and (2) a diffusion model is trained to produce the next frame, conditioned on the sequence of past frames and actions. Conditioning augmentations help ensure stable auto-regressive generation over long trajectories, and decoder fine-tuning improves the fidelity of visual details and text.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion},
  file = {/Users/frankcholula/Zotero/storage/KHRTI2N2/Valevski et al. - 2025 - Diffusion Models Are Real-Time Game Engines.pdf;/Users/frankcholula/Zotero/storage/9HVS7KPP/2408.html}
}

@article{vincentConnectionScoreMatching2011,
  title = {A {{Connection Between Score Matching}} and {{Denoising Autoencoders}}},
  author = {Vincent, Pascal},
  date = {2011-07},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {23},
  number = {7},
  pages = {1661--1674},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00142},
  url = {https://direct.mit.edu/neco/article/23/7/1661-1674/7677},
  urldate = {2025-04-27},
  abstract = {Denoising autoencoders have been previously shown to be competitive alternatives to Restricted Boltzmann Machines for unsupervised pre-training of each layer of a deep architecture. We show that a simple denoising autoencoder training criterion is equivalent to matching the score (with respect to the data) of a specific energy based model to that of a non-parametric Parzen density estimator of the data. This yields several useful insights. It defines a proper probabilistic model for the denoising autoencoder technique which makes it in principle possible to sample from them or to rank examples by their energy. It suggests a different way to apply score matching that is related to learning to denoise and does not require computing second derivatives. It justifies the use of tied weights between the encoder and decoder, and suggests ways to extend the success of denoising autoencoders to a larger family of energy-based models.},
  langid = {english},
  keywords = {notion},
  file = {/Users/frankcholula/Zotero/storage/7KNDKJEW/Vincent - 2011 - A Connection Between Score Matching and Denoising Autoencoders.pdf}
}

@online{zhangUnreasonableEffectivenessDeep2018,
  title = {The {{Unreasonable Effectiveness}} of {{Deep Features}} as a {{Perceptual Metric}}},
  author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
  date = {2018-04-10},
  eprint = {1801.03924},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1801.03924},
  url = {http://arxiv.org/abs/1801.03924},
  urldate = {2025-05-06},
  abstract = {While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,notion},
  file = {/Users/frankcholula/Zotero/storage/PBKMD2PD/Zhang et al. - 2018 - The Unreasonable Effectiveness of Deep Features as a Perceptual Metric.pdf;/Users/frankcholula/Zotero/storage/KTQQ6G3L/1801.html}
}

@online{zhuDiffusionModelsReinforcement2024,
  title = {Diffusion {{Models}} for {{Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Diffusion {{Models}} for {{Reinforcement Learning}}},
  author = {Zhu, Zhengbang and Zhao, Hanye and He, Haoran and Zhong, Yichao and Zhang, Shenyu and Guo, Haoquan and Chen, Tingting and Zhang, Weinan},
  date = {2024-02-23},
  eprint = {2311.01223},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.01223},
  url = {http://arxiv.org/abs/2311.01223},
  urldate = {2025-05-03},
  abstract = {Diffusion models surpass previous generative models in sample quality and training stability. Recent works have shown the advantages of diffusion models in improving reinforcement learning (RL) solutions. This survey aims to provide an overview of this emerging field and hopes to inspire new avenues of research. First, we examine several challenges encountered by RL algorithms. Then, we present a taxonomy of existing methods based on the roles of diffusion models in RL and explore how the preceding challenges are addressed. We further outline successful applications of diffusion models in various RL-related tasks. Finally, we conclude the survey and offer insights into future research directions. We are actively maintaining a GitHub repository for papers and other related resources in utilizing diffusion models in RL: https://github.com/apexrl/Diff4RLSurvey.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,notion},
  file = {/Users/frankcholula/Zotero/storage/BWXCCFCG/Zhu et al. - 2024 - Diffusion Models for Reinforcement Learning A Survey.pdf;/Users/frankcholula/Zotero/storage/BYKXSQJX/2311.html}
}

@online{parmarAliasedResizingSurprising2022,
  title = {On {{Aliased Resizing}} and {{Surprising Subtleties}} in {{GAN Evaluation}}},
  author = {Parmar, Gaurav and Zhang, Richard and Zhu, Jun-Yan},
  date = {2022-01-21},
  eprint = {2104.11222},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2104.11222},
  url = {http://arxiv.org/abs/2104.11222},
  urldate = {2025-05-12},
  abstract = {Metrics for evaluating generative models aim to measure the discrepancy between real and generated images. The often-used Frechet Inception Distance (FID) metric, for example, extracts "high-level" features using a deep network from the two sets. However, we find that the differences in "low-level" preprocessing, specifically image resizing and compression, can induce large variations and have unforeseen consequences. For instance, when resizing an image, e.g., with a bilinear or bicubic kernel, signal processing principles mandate adjusting prefilter width depending on the downsampling factor, to antialias to the appropriate bandwidth. However, commonly-used implementations use a fixed-width prefilter, resulting in aliasing artifacts. Such aliasing leads to corruptions in the feature extraction downstream. Next, lossy compression, such as JPEG, is commonly used to reduce the file size of an image. Although designed to minimally degrade the perceptual quality of an image, the operation also produces variations downstream. Furthermore, we show that if compression is used on real training images, FID can actually improve if the generated images are also subsequently compressed. This paper shows that choices in low-level image processing have been an underappreciated aspect of generative modeling. We identify and characterize variations in generative modeling development pipelines, provide recommendations based on signal processing principles, and release a reference implementation to facilitate future comparisons.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,notion},
  file = {/Users/frankcholula/Zotero/storage/BAPJ33FK/Parmar et al. - 2022 - On Aliased Resizing and Surprising Subtleties in GAN Evaluation.pdf;/Users/frankcholula/Zotero/storage/T2E8296H/2104.html}
}

@online{razaviGeneratingDiverseHighFidelity2019,
  title = {Generating {{Diverse High-Fidelity Images}} with {{VQ-VAE-2}}},
  author = {Razavi, Ali and van den Oord, Aaron and Vinyals, Oriol},
  date = {2019-06-02},
  eprint = {1906.00446},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1906.00446},
  url = {http://arxiv.org/abs/1906.00446},
  urldate = {2025-05-12},
  abstract = {We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/frankcholula/Zotero/storage/52ZZUZRD/Razavi et al. - 2019 - Generating Diverse High-Fidelity Images with VQ-VAE-2.pdf;/Users/frankcholula/Zotero/storage/G2F3C58Q/1906.html}
}
@online{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014-06-10},
  eprint = {1406.2661},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1406.2661},
  url = {http://arxiv.org/abs/1406.2661},
  urldate = {2025-05-12},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/frankcholula/Zotero/storage/P8544FUY/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf;/Users/frankcholula/Zotero/storage/ZHXSLXLB/1406.html}
}
@online{karrasProgressiveGrowingGANs2018a,
  title = {Progressive {{Growing}} of {{GANs}} for {{Improved Quality}}, {{Stability}}, and {{Variation}}},
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  date = {2018-02-26},
  eprint = {1710.10196},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1710.10196},
  url = {http://arxiv.org/abs/1710.10196},
  urldate = {2025-05-12},
  abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024\textasciicircum 2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,notion,Statistics - Machine Learning}
}
@online{chenNitroFusionHighFidelitySingleStep2024,
  title = {{{NitroFusion}}: {{High-Fidelity Single-Step Diffusion}} through {{Dynamic Adversarial Training}}},
  shorttitle = {{{NitroFusion}}},
  author = {Chen, Dar-Yen and Bandyopadhyay, Hmrishav and Zou, Kai and Song, Yi-Zhe},
  date = {2024-12-06},
  eprint = {2412.02030},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2412.02030},
  url = {http://arxiv.org/abs/2412.02030},
  urldate = {2025-05-12},
  abstract = {We introduce NitroFusion, a fundamentally different approach to single-step diffusion that achieves high-quality generation through a dynamic adversarial framework. While one-step methods offer dramatic speed advantages, they typically suffer from quality degradation compared to their multi-step counterparts. Just as a panel of art critics provides comprehensive feedback by specializing in different aspects like composition, color, and technique, our approach maintains a large pool of specialized discriminator heads that collectively guide the generation process. Each discriminator group develops expertise in specific quality aspects at different noise levels, providing diverse feedback that enables high-fidelity one-step generation. Our framework combines: (i) a dynamic discriminator pool with specialized discriminator groups to improve generation quality, (ii) strategic refresh mechanisms to prevent discriminator overfitting, and (iii) global-local discriminator heads for multi-scale quality assessment, and unconditional/conditional training for balanced generation. Additionally, our framework uniquely supports flexible deployment through bottom-up refinement, allowing users to dynamically choose between 1-4 denoising steps with the same model for direct quality-speed trade-offs. Through comprehensive experiments, we demonstrate that NitroFusion significantly outperforms existing single-step methods across multiple evaluation metrics, particularly excelling in preserving fine details and global consistency.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {/Users/frankcholula/Zotero/storage/4TUTIHSZ/Chen et al. - 2024 - NitroFusion High-Fidelity Single-Step Diffusion through Dynamic Adversarial Training.pdf;/Users/frankcholula/Zotero/storage/TAM2YUL5/2412.html}
}
@online{girdharImageBindOneEmbedding2023,
  title = {{{ImageBind}}: {{One Embedding Space To Bind Them All}}},
  shorttitle = {{{ImageBind}}},
  author = {Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
  date = {2023-05-31},
  eprint = {2305.05665},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.05665},
  url = {http://arxiv.org/abs/2305.05665},
  urldate = {2025-05-12},
  abstract = {We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia,notion},
  file = {/Users/frankcholula/Zotero/storage/T4VGHTUY/Girdhar et al. - 2023 - ImageBind One Embedding Space To Bind Them All.pdf;/Users/frankcholula/Zotero/storage/9DSDN9J5/2305.html}
}
@online{lengREPAEUnlockingVAE2025,
  title = {{{REPA-E}}: {{Unlocking VAE}} for {{End-to-End Tuning}} with {{Latent Diffusion Transformers}}},
  shorttitle = {{{REPA-E}}},
  author = {Leng, Xingjian and Singh, Jaskirat and Hou, Yunzhong and Xing, Zhenchang and Xie, Saining and Zheng, Liang},
  date = {2025-04-14},
  eprint = {2504.10483},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2504.10483},
  url = {http://arxiv.org/abs/2504.10483},
  urldate = {2025-05-12},
  abstract = {In this paper we tackle a fundamental question: "Can we train latent diffusion models together with the variational auto-encoder (VAE) tokenizer in an end-to-end manner?" Traditional deep-learning wisdom dictates that end-to-end training is often preferable when possible. However, for latent diffusion transformers, it is observed that end-to-end training both VAE and diffusion-model using standard diffusion-loss is ineffective, even causing a degradation in final performance. We show that while diffusion loss is ineffective, end-to-end training can be unlocked through the representation-alignment (REPA) loss -- allowing both VAE and diffusion model to be jointly tuned during the training process. Despite its simplicity, the proposed training recipe (REPA-E) shows remarkable performance; speeding up diffusion model training by over 17x and 45x over REPA and vanilla training recipes, respectively. Interestingly, we observe that end-to-end tuning with REPA-E also improves the VAE itself; leading to improved latent space structure and downstream generation performance. In terms of final performance, our approach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and without classifier-free guidance on ImageNet 256 x 256. Code is available at https://end2end-diffusion.github.io.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion},
  file = {/Users/frankcholula/Zotero/storage/Y3N4QY8I/Leng et al. - 2025 - REPA-E Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers.pdf;/Users/frankcholula/Zotero/storage/73U5HGJ8/2504.html}
}
